{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3711jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462",
   "display_name": "Python 3.7.11 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Aspect-Based Sentiment Analysis (ABSA)Â¶\n",
    "This notebook contains the code for the second homework of NLP course 2021 at Sapienza, University of Rome.\n",
    "Author: Leonardo Emili (1802989)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup the environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#@title General settings\n",
    "#@markdown ##### If using GDrive to copy files, make sure to name them correctly.\n",
    "copy_from_drive = True #@param {type:\"boolean\"}\n",
    "if copy_from_drive:\n",
    "    #!cp /content/gdrive/MyDrive/GoogleNews-vectors-negative300.txt ../../data/\n",
    "    !cp /content/drive/MyDrive/w2v_weights.pth ../../data/\n",
    "\n",
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wed Sep  1 22:21:20 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies\n",
    "Setup the environment downloading the required resources, the evaluation tool used for this homework, and configure the logger to have useful plots."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install -q wandb pytorch-lightning transformers axial-positional-embedding pytorch-crf\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "from dataset import ABSADataset\n",
    "from hw2.evaluate import evaluate_sentiment\n",
    "from utils import HParams, log_n_samples, download_nltk_resources, pl_trainer, Vectors\n",
    "\n",
    "download_nltk_resources()\n",
    "cached_vectors: Vectors = Vectors.from_cached('../../data/w2v_weights.pth')\n",
    "\n",
    "import wandb\n",
    "project = 'nlp_hw2'\n",
    "wandb.login()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "ERROR: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleonardoemili\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task A - Aspect term identification\n",
    "In this section, various approaches are shown for aspect terms extraction using ground truth labeled data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Baseline model (LSTM)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from utils import simple_collate_fn\n",
    "ds = ABSADataset.from_path(collate_fn=simple_collate_fn)\n",
    "\n",
    "run = wandb.init(reinit=True, project=project, tags=['lstm'])\n",
    "hparams = HParams(\n",
    "    ds.ner_vocab,\n",
    "    input_dim=ds.feature_size,\n",
    "    hidden_dim=512,\n",
    "    epochs=30,\n",
    "    dropout=0.5,\n",
    "    lr=0.2,\n",
    "    model_name='lstm'\n",
    ")\n",
    "model = NERClassifier(hparams)\n",
    "\n",
    "trainer = trainer = pl_trainer(max_epochs=hparams.epochs)\n",
    "trainer.fit(model, ds)\n",
    "trainer.test()\n",
    "\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BERT-based model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models.ner_classifier import NERClassifier\n",
    "from transformers import BertTokenizer\n",
    "from utils import collate_fn\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.from_path(collate_fn=collate_fn, tokenizer=BertTokenizer.from_pretrained(bert_name))\n",
    "\n",
    "run = wandb.init(reinit=True, project=project, tags=['bert_lstm', bert_name])\n",
    "hparams = HParams(\n",
    "    ds.ner_vocab,\n",
    "    hidden_dim=300,\n",
    "    epochs=30,\n",
    "    dropout=0.6,\n",
    "    lr=0.1,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name\n",
    ")\n",
    "model = NERClassifier(hparams)\n",
    "\n",
    "trainer = pl_trainer(max_epochs=hparams.epochs)\n",
    "trainer.fit(model, ds)\n",
    "trainer.test()\n",
    "\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pretrained NER classifier (HF)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "from utils import collate_fn\n",
    "\n",
    "bert_name = 'dslim/bert-base-NER'\n",
    "ds = ABSADataset.from_path(collate_fn=collate_fn, tokenizer=AutoTokenizer.from_pretrained(bert_name))\n",
    "\n",
    "run = wandb.init(reinit=True, project=project, tags=['bert_lstm'])\n",
    "hparams = HParams(\n",
    "    ds.ner_vocab,\n",
    "    hidden_dim=512,\n",
    "    epochs=30,\n",
    "    dropout=0.65,\n",
    "    lr=0.1,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name\n",
    ")\n",
    "model = NERClassifier(hparams)\n",
    "\n",
    "trainer = pl_trainer(max_epochs=hparams.epochs)\n",
    "trainer.fit(model, ds)\n",
    "trainer.test()\n",
    "\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task B - Aspect term polarity classification\n",
    "In this section, polarity classification of extracted aspect terms is performed using a dedicated model for the task."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer\n",
    "from utils import collate_fn\n",
    "from models.polarity_classifier import PolarityClassifier\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.from_path(collate_fn=collate_fn, tokenizer=BertTokenizer.from_pretrained(bert_name))\n",
    "\n",
    "run = wandb.init(reinit=True, project=project, tags=['bert_lstm', bert_name])\n",
    "hparams = HParams(\n",
    "    ds.polarity_vocab,\n",
    "    hidden_dim=100,\n",
    "    epochs=30,\n",
    "    dropout=0.6,\n",
    "    lr=0.2,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name\n",
    ")\n",
    "model = PolarityClassifier(hparams)\n",
    "\n",
    "trainer = pl_trainer(max_epochs=hparams.epochs)\n",
    "trainer.fit(model, ds)\n",
    "trainer.test()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aspect identification pipeline - Task A+B\n",
    "In this section, we address both aspect terms extraction and polarity classification using a single model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer\n",
    "from models.absa_classifier import ABSAClassifier\n",
    "from utils import collate_fn\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.from_path(\n",
    "    merge_dev_sets=True,\n",
    "    collate_fn=collate_fn,\n",
    "    tokenizer=BertTokenizer.from_pretrained(bert_name),\n",
    "    extended_bio=True\n",
    ")\n",
    "\n",
    "run = wandb.init(reinit=True, project=project, tags=['bert_lstm', bert_name])\n",
    "hparams = HParams(\n",
    "    ds.ner_ext_vocab,\n",
    "    hidden_dim=300,\n",
    "    epochs=30,\n",
    "    dropout=0.6,\n",
    "    lr=0.1,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name\n",
    ")\n",
    "model = ABSAClassifier(hparams)\n",
    "model.evaluate_callback = evaluate_sentiment\n",
    "\n",
    "trainer = pl_trainer(max_epochs=hparams.epochs)\n",
    "trainer.fit(model, ds)\n",
    "trainer.test()\n",
    "\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Category identification pipeline - Task C+D\n",
    "In this section, we address both category terms extraction and polarity classification using a single model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer\n",
    "from models.category_classifier import CategoryClassifier\n",
    "from utils import collate_fn\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.restaurants_from_path(\n",
    "    collate_fn=collate_fn,\n",
    "    tokenizer=BertTokenizer.from_pretrained(bert_name)\n",
    ")\n",
    "\n",
    "run = wandb.init(reinit=True, project=project, tags=['bert_lstm', bert_name])\n",
    "hparams = HParams(\n",
    "    ds.category_ext_vocab,\n",
    "    hidden_dim=300,\n",
    "    epochs=30,\n",
    "    dropout=0.6,\n",
    "    lr=0.2,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name\n",
    ")\n",
    "model = CategoryClassifier(hparams, evaluate_callback=evaluate_sentiment)\n",
    "\n",
    "trainer = pl_trainer(max_epochs=hparams.epochs)\n",
    "trainer.fit(model, ds)\n",
    "trainer.test()\n",
    "\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ABSA pipeline - A+B+C+D\n",
    "In this section, we apply **multitask learning** (Caruana, 1996) to solve both tasks A+B and C+D. More details on the implementation in the report."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer\n",
    "from models.multistep_classifier import MultistepClassifier\n",
    "from utils import collate_fn\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.from_path(\n",
    "    merge_dev_sets=True,\n",
    "    collate_fn=collate_fn,\n",
    "    tokenizer=BertTokenizer.from_pretrained(bert_name),\n",
    "    extended_bio=True,\n",
    "    use_class_weights=True,\n",
    "    cached_vectors=cached_vectors,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "run = wandb.init(reinit=True, project=project, tags=['bert_lstm', bert_name])\n",
    "ab_hparams = HParams(\n",
    "    ds.ner_ext_vocab,\n",
    "    hidden_dim=512,\n",
    "    epochs=50,\n",
    "    dropout=0.65,\n",
    "    lr=0.05,\n",
    "    bert_name=bert_name,\n",
    "    sentence_encoder='lstm'\n",
    ")\n",
    "\n",
    "cd_hparams = HParams(ds.polarity_vocab)\n",
    "\n",
    "model = MultistepClassifier(ab_hparams, cd_hparams, ds.category_vocab, pos_vocab=ds.pos_vocab, evaluate_callback=evaluate_sentiment, **ds.configs)\n",
    "trainer = pl_trainer(max_epochs=ab_hparams.epochs, monitor='trainer/val_aspect_sentiment_f1_macro', mode='max', precision=16)\n",
    "trainer.fit(model, ds)\n",
    "trainer.test()\n",
    "\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference test\n",
    "In this section, we can perform testing using models trained at previous steps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task A"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models.ner_classifier import NERClassifier\n",
    "from transformers import BertTokenizer\n",
    "from utils import collate_fn\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.from_path(collate_fn=collate_fn, tokenizer=BertTokenizer.from_pretrained(bert_name))\n",
    "hparams = HParams(ds.ner_vocab, input_dim=ds.feature_size, hidden_dim=512, model_name='lstm')\n",
    "best_model_path = 'checkpoints/epoch=11-step=119.ckpt'\n",
    "ner_model = NERClassifier.load_from_checkpoint(best_model_path, hparams=hparams)\n",
    "trainer = pl_trainer()\n",
    "trainer.test(ner_model, ds.test_dataloader())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task B"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer\n",
    "from utils import collate_fn\n",
    "from models.polarity_classifier import PolarityClassifier\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.from_path(collate_fn=collate_fn, tokenizer=BertTokenizer.from_pretrained(bert_name))\n",
    "hparams = HParams(ds.polarity_vocab, hidden_dim=100, model_name='bert_lstm', bert_name=bert_name)\n",
    "best_model_path = 'checkpoints/polarity_test.ckpt'\n",
    "polarity_model = PolarityClassifier.load_from_checkpoint(best_model_path, hparams=hparams)\n",
    "trainer = pl_trainer()\n",
    "trainer.test(polarity_model, ds.test_dataloader())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task A+B"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer\n",
    "from models.absa_classifier import ABSAClassifier\n",
    "from utils import collate_fn\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.from_path(\n",
    "    merge_dev_sets=False,\n",
    "    collate_fn=collate_fn,\n",
    "    tokenizer=BertTokenizer.from_pretrained(bert_name),\n",
    "    extended_bio=True\n",
    ")\n",
    "\n",
    "hparams = HParams(\n",
    "    ds.ner_ext_vocab,\n",
    "    hidden_dim=300,\n",
    "    epochs=30,\n",
    "    dropout=0.6,\n",
    "    lr=0.1,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name\n",
    ")\n",
    "\n",
    "best_model_path = 'checkpoints/absa_test_3.ckpt'\n",
    "best_model_path = 'checkpoints/epoch=11-step=239.ckpt'\n",
    "model = ABSAClassifier.load_from_checkpoint(best_model_path, hparams=hparams)\n",
    "\n",
    "trainer = pl_trainer(max_epochs=hparams.epochs)\n",
    "trainer.test(model, ds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models.absa_classifier import AspectMultistepClassifier, model_from\n",
    "from transformers import BertTokenizer\n",
    "from utils import collate_fn\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.from_path(\n",
    "    collate_fn=collate_fn,\n",
    "    tokenizer=BertTokenizer.from_pretrained(bert_name),\n",
    "    extended_bio=True\n",
    ")\n",
    "\n",
    "run = wandb.init(reinit=True, project=project, tags=['bert_lstm', bert_name])\n",
    "ner_hparams = HParams(\n",
    "    ds.ner_vocab,\n",
    "    hidden_dim=300,\n",
    "    epochs=30,\n",
    "    dropout=0.6,\n",
    "    lr=0.1,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name,\n",
    "    ner_model_path='checkpoints/ner_classifier.ckpt'\n",
    ")\n",
    "polarity_hparams = HParams(\n",
    "    ds.polarity_vocab,\n",
    "    hidden_dim=100,\n",
    "    epochs=30,\n",
    "    dropout=0.6,\n",
    "    lr=0.2,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name,\n",
    "    polarity_model_path='checkpoints/polarity_classifier.ckpt'\n",
    ")\n",
    "\n",
    "model = model_from(ner_hparams, polarity_hparams)\n",
    "\n",
    "trainer = pl_trainer(max_epochs=ner_hparams.epochs)\n",
    "trainer.test(model, ds.test_dataloader())\n",
    "\n",
    "run.finish()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task C+D"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer\n",
    "from models.category_classifier import CategoryClassifier\n",
    "from utils import collate_fn\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.restaurants_from_path(\n",
    "    collate_fn=collate_fn,\n",
    "    tokenizer=BertTokenizer.from_pretrained(bert_name)\n",
    ")\n",
    "\n",
    "hparams = HParams(\n",
    "    ds.category_ext_vocab,\n",
    "    hidden_dim=300,\n",
    "    epochs=30,\n",
    "    dropout=0.6,\n",
    "    lr=0.1,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name\n",
    ")\n",
    "\n",
    "best_model_path = 'checkpoints/epoch=29-step=299.ckpt'\n",
    "model = CategoryClassifier.load_from_checkpoint(best_model_path, hparams=hparams, evaluate_callback=evaluate_sentiment)\n",
    "trainer = pl_trainer(max_epochs=hparams.epochs)\n",
    "trainer.test(model, ds)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multistep Testing (A+B+C+D)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer\n",
    "from models.multistep_classifier import MultistepClassifier\n",
    "from utils import collate_fn\n",
    "\n",
    "bert_name = 'bert-base-uncased'\n",
    "ds = ABSADataset.restaurants_from_path(\n",
    "    collate_fn=collate_fn,\n",
    "    tokenizer=BertTokenizer.from_pretrained(bert_name),\n",
    "    extended_bio=True\n",
    ")\n",
    "\n",
    "ab_hparams = HParams(\n",
    "    ds.ner_ext_vocab,\n",
    "    hidden_dim=512,\n",
    "    epochs=30,\n",
    "    dropout=0.6,\n",
    "    lr=0.1,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name\n",
    ")\n",
    "\n",
    "cd_hparams = HParams(\n",
    "    ds.polarity_vocab,\n",
    "    hidden_dim=512,\n",
    "    epochs=30,\n",
    "    dropout=0.6,\n",
    "    lr=0.1,\n",
    "    model_name='bert_lstm',\n",
    "    bert_name=bert_name\n",
    ")\n",
    "\n",
    "model = MultistepClassifier.load_from_checkpoint(\n",
    "    'checkpoints/epoch=15-step=319.ckpt',\n",
    "    ab_hparams=ab_hparams,\n",
    "    cd_hparams=cd_hparams,\n",
    "    category_vocab=ds.category_vocab,\n",
    "    pos_vocab=ds.pos_vocab,\n",
    "    evaluate_callback=evaluate_sentiment,\n",
    "    strict=False\n",
    ")\n",
    "trainer = pl_trainer(max_epochs=ab_hparams.epochs)\n",
    "trainer.test(model, ds)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}